{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "should_skip = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not should_skip:\n",
    "  !pip install -U -qq git+https://github.com/huggingface/diffusers.git\n",
    "  !pip install -qq accelerate transformers ftfy\n",
    "  !pip install gpustat gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Import required libraries\n",
    "import argparse\n",
    "import itertools\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TVF\n",
    "import torch.utils.checkpoint\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import PIL\n",
    "from accelerate import Accelerator\n",
    "from accelerate.logging import get_logger\n",
    "from accelerate.utils import set_seed\n",
    "\n",
    "# https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/unet_2d_condition.py#L175-L182\n",
    "from diffusers import AutoencoderKL, DDPMScheduler, PNDMScheduler, StableDiffusionPipeline, UNet2DConditionModel\n",
    "from diffusers.optimization import get_scheduler\n",
    "from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import CLIPFeatureExtractor, CLIPTextModel, CLIPTokenizer\n",
    "\n",
    "def image_grid(imgs, rows, cols):\n",
    "    assert len(imgs) == rows*cols\n",
    "\n",
    "    w, h = imgs[0].size\n",
    "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
    "    grid_w, grid_h = grid.size\n",
    "\n",
    "    for i, img in enumerate(imgs):\n",
    "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not should_skip:\n",
    "    #@markdown Add here the URLs to the images of the concept you are adding. 3-5 should be fine\n",
    "    urls = [\n",
    "        \"https://huggingface.co/datasets/valhalla/images/resolve/main/2.jpeg\",\n",
    "        \"https://huggingface.co/datasets/valhalla/images/resolve/main/3.jpeg\",\n",
    "        # \"https://huggingface.co/datasets/valhalla/images/resolve/main/5.jpeg\",\n",
    "        # \"https://huggingface.co/datasets/valhalla/images/resolve/main/6.jpeg\",\n",
    "        ## You can add additional images here\n",
    "    ]\n",
    "#@title Download\n",
    "import requests\n",
    "import glob\n",
    "from io import BytesIO\n",
    "\n",
    "def download_image(url):\n",
    "  try:\n",
    "    response = requests.get(url)\n",
    "  except:\n",
    "    return None\n",
    "  return Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "\n",
    "if not should_skip:\n",
    "    images = list(filter(None,[download_image(url) for url in urls]))\n",
    "    save_path = \"./my_concept\"\n",
    "    if not os.path.exists(save_path):\n",
    "    os.mkdir(save_path)\n",
    "    [image.save(f\"{save_path}/{i}.jpeg\") for i, image in enumerate(images)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not should_skip:\n",
    "    !gdown --id 1I8RMKQhPXntlRKorVNZ6yyNpfaOp8wNv\n",
    "    mask_path=\"./my_concept_gt\"\n",
    "    !mkdir $mask_path/\n",
    "\n",
    "    with open(\"my_concept_gt.zip\",\"r\") as f:\n",
    "        pass\n",
    "    !unzip -o my_concept_gt.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from slime.slime import Slime\n",
    "from slime.data.single_class import BinarySegmentationDataset,SegmentationDataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = BinarySegmentationDataset(\n",
    "    data_root=\"my_concept\",\n",
    "    mask_root=\"my_concept_gt\",\n",
    ")\n",
    "\n",
    "data_module = SegmentationDataModule(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slime = Slime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    default_root_dir=\"run\",\n",
    "    limit_train_batches=1,\n",
    "    limit_val_batches=1,\n",
    "    num_sanity_val_steps=0,\n",
    "    max_epochs=50,\n",
    "    weights_summary=None,\n",
    ")\n",
    "\n",
    "trainer.fit(slime, datamodule=data_module)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
